{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1QEGU2voSxYQ0tYRgISaGT52KYQRuHeG0",
      "authorship_tag": "ABX9TyP9wWe6a8Gh+/gkMPcjsgA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeanuInterone/AutonomousDrivingGamePlayer/blob/main/AutonoumouseGamePlayerModelTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ¤– Self Driving Model Trainer ðŸ¤–"
      ],
      "metadata": {
        "id": "JTH5MhbG-afy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sG8BurxN-s5B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nq0Y7Iou-Wo_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure"
      ],
      "metadata": {
        "id": "G_YOyjlQ_b4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (20, 256, 256, 3)\n",
        "num_classes = 4\n",
        "batch_size = 32\n",
        "traing_batched_files = \"/content/drive/MyDrive/Projects/AutonomousGamePlayer/BatchedExamples/train\"\n",
        "validation_batched_files = \"/content/drive/MyDrive/Projects/AutonomousGamePlayer/BatchedExamples/val\"\n",
        "test_batched_files = \"/content/drive/MyDrive/Projects/AutonomousGamePlayer/BatchedExamples/test\"\n",
        "checkpoints_path = \"/content/drive/MyDrive/Projects/AutonomousGamePlayer/ModelCheckpoints\""
      ],
      "metadata": {
        "id": "2oWO_2M3_s70"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Pickle File Paths"
      ],
      "metadata": {
        "id": "gx4dOzKHGXTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pickle_file_paths(dir):\n",
        "  \"\"\"\n",
        "  Gets the list of pickle files in a directory\n",
        "  ARGS:\n",
        "    dir (string): Directory\n",
        "  RETURNS:\n",
        "    file_names (list): List of file names\n",
        "  \"\"\"\n",
        "  file_names = []\n",
        "  for filename in os.listdir(dir):\n",
        "    if filename.endswith('.pickle'):\n",
        "      path = os.path.join(dir, filename)\n",
        "      file_names.append(path)\n",
        "  return file_names"
      ],
      "metadata": {
        "id": "gxxAfSohGW79"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pickle Object"
      ],
      "metadata": {
        "id": "539hYFE5-7Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle_object(file_name):\n",
        "  \"\"\"\n",
        "  Loads a pickle file\n",
        "  ARGS:\n",
        "    file_name (string): File name\n",
        "  RETURNS:\n",
        "    object (dict): the pickled object\n",
        "  \"\"\"\n",
        "  # Open file\n",
        "  with open(file_name, 'rb') as handle:\n",
        "    # Load data\n",
        "    pickle_object = pickle.load(handle)\n",
        "    return pickle_object"
      ],
      "metadata": {
        "id": "GSPxp1RH-7EP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create X and y from examples"
      ],
      "metadata": {
        "id": "N97j5xDRBSee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_X_y_from_examples(examples):\n",
        "  \"\"\"\n",
        "  Create X and y from examples\n",
        "  ARGS:\n",
        "    examples (list): the list of examples\n",
        "  RETURNS:\n",
        "    X (np array): the list of features\n",
        "    y (np array): the list of labels\n",
        "  \"\"\"\n",
        "  X = []\n",
        "  y = []\n",
        "  for example in examples:\n",
        "    X.append(example['x_screen_frames'] * 255)\n",
        "    y.append(example['y_key_state'])\n",
        "  np_X = np.array(X)\n",
        "  np_y = np.array(y)\n",
        "  del X\n",
        "  del y\n",
        "  return np_X, np_y"
      ],
      "metadata": {
        "id": "izUK6U46BSCO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Example Generator"
      ],
      "metadata": {
        "id": "0v-HLtknB2zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_example_generator(batch_file_paths, create_x_y_func):\n",
        "  \"\"\"\n",
        "  Batch example generator\n",
        "  ARGS:\n",
        "    batch_file_paths (list): the list of batch file paths\n",
        "    create_x_y_func (function): the function to create X and y from examples\n",
        "  RETURNS:\n",
        "    example (dict): the example\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    for batch_file_path in batch_file_paths:\n",
        "      examples = load_pickle_object(batch_file_path)\n",
        "      Xs, ys = create_x_y_func(examples)\n",
        "      yield Xs, ys"
      ],
      "metadata": {
        "id": "caSl1VIBB6bZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "JdUWALtZEEyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_data_dir,\n",
        "    val_data_dir,\n",
        "    create_X_y_from_examples_func,\n",
        "    checkpoint_path,\n",
        "    batch_size,\n",
        "    data_portion = 1.0\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Train model\n",
        "  ARGS:\n",
        "    model\n",
        "    model_name\n",
        "    train_data_dir\n",
        "    val_data_dir\n",
        "    create_X_y_from_examples_func\n",
        "    checkpoint_path\n",
        "    batch_size\n",
        "    data_portion\n",
        "  \"\"\"\n",
        "  # Checkpoints\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=f\"{checkpoint_path}/{model_name}/model_checkpoint.h5\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only=False,\n",
        "      monitor='val_loss',\n",
        "      mode='min',\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "  # Early stoping\n",
        "  early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      patience=2,\n",
        "      verbose=1,\n",
        "      restore_best_weights=True\n",
        "  )\n",
        "\n",
        "  # Get Train and Val files\n",
        "  train_batch_file_paths = get_pickle_file_paths(train_data_dir)\n",
        "  val_batch_file_paths = get_pickle_file_paths(val_data_dir)\n",
        "\n",
        "  # Portion Train and Val sets\n",
        "  train_batch_file_paths = train_batch_file_paths[:int(len(train_batch_file_paths) * data_portion)]\n",
        "  val_batch_file_paths = val_batch_file_paths[:int(len(val_batch_file_paths) * data_portion)]\n",
        "\n",
        "  # Create generators\n",
        "  train_generator = batch_example_generator(train_batch_file_paths, create_X_y_from_examples_func)\n",
        "  val_generator = batch_example_generator(val_batch_file_paths, create_X_y_from_examples_func)\n",
        "\n",
        "  # Fit the model with callbacks\n",
        "  model.fit(\n",
        "      train_generator,\n",
        "      batch_size=batch_size,\n",
        "      steps_per_epoch=len(train_batch_file_paths),\n",
        "      epochs=10,\n",
        "      validation_data=val_generator,\n",
        "      validation_steps=len(val_batch_file_paths),\n",
        "      callbacks=[checkpoint_callback, early_stopping_callback]\n",
        "  )"
      ],
      "metadata": {
        "id": "0XWBSWE9EV7a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "N_Da5MM1CmcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, num_classes):\n",
        "  # Define input layers\n",
        "  video_input = tf.keras.layers.Input(shape=input_shape, name='video_input')\n",
        "\n",
        "  # Resize\n",
        "  image_size = 96 # in [96, 128, 160, 192, 224]\n",
        "  video_input_resized = tf.keras.layers.TimeDistributed(tf.keras.layers.Resizing(image_size, image_size), name='resize')(video_input)\n",
        "\n",
        "  # Preprosess for ResNet\n",
        "  video_input_preprocess = tf.keras.applications.mobilenet_v2.preprocess_input(video_input_resized)\n",
        "\n",
        "  # Load MobileNet base\n",
        "  mobilenet_base = tf.keras.applications.MobileNetV2(\n",
        "      include_top=False,\n",
        "      weights='imagenet',\n",
        "      input_shape=(image_size, image_size, 3),\n",
        "      pooling='avg'\n",
        "  )\n",
        "  mobilenet_base.trainable = False\n",
        "\n",
        "  # Image features\n",
        "  image_features = tf.keras.layers.TimeDistributed(mobilenet_base, name='image_features')(video_input_preprocess)\n",
        "\n",
        "  # LSTM layer for video temporal modeling\n",
        "  video_lstm_features = tf.keras.layers.LSTM(128, return_sequences=True)(image_features)\n",
        "  video_lstm_output = tf.keras.layers.LSTM(128, return_sequences=False)(video_lstm_features)\n",
        "\n",
        "  # Fully connected layer for prediction\n",
        "  output = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='output')(video_lstm_output)\n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.models.Model(inputs=video_input, outputs=output)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Return\n",
        "  return model"
      ],
      "metadata": {
        "id": "UlzxTw1tCPpE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(input_shape, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Vs4wWKwYO3Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    model,\n",
        "    'model_1',\n",
        "    traing_batched_files,\n",
        "    validation_batched_files,\n",
        "    create_X_y_from_examples,\n",
        "    checkpoints_path,\n",
        "    batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "BAKmIxASO6X5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}